playerName = tmphtml %>% html_nodes(xpath = "/html/body/div[@id='container']/div[@id='main']/h2[@class='pname']") %>% html_text()
playerInformation = tmphtml %>% html_nodes(xpath = "/html/body/div[@id='container']/div[@id='main']/table[@class='player-info']//td") %>% html_text() %>% as.vector()
playerInformationAndRecord = c(playerName, playerInformation, lastYearRecord) %>% as.matrix()
}
if (!is.null(playerInformationAndRecord)){
playerInformationAndRecordAll = cbind(playerInformationAndRecordAll, playerInformationAndRecord)
Sys.sleep(2)
}
}
PplayerInformationAndRecordAll = t( playerInformationAndRecordAll)
# データを整形する
# データのヘッダー部分を取得する
HtableHead1 = read_html(urlListAllH[1]) %>% html_nodes(xpath = "/html/body/div[@id='container']/div[@id='main']/table[@class='player-info']//th") %>% html_text() %>% as.vector()
HtableHead2 = read_html(urlListAllH[1])%>% html_nodes(xpath = "/html/body/div[@id='container']/div[@id='main']/table[@id='tbl']//th") %>% html_text() %>% as.vector()
Sys.sleep(2)
PtableHead1 = read_html(urlListAllP[1]) %>% html_nodes(xpath = "/html/body/div[@id='container']/div[@id='main']/table[@class='player-info']//th") %>% html_text() %>% as.vector()
PtableHead2 = read_html(urlListAllP[1])%>% html_nodes(xpath = "/html/body/div[@id='container']/div[@id='main']/table[@id='tbl']//th") %>% html_text() %>% as.vector()
colnames(PplayerInformationAndRecordAll) = c(PtableHead1,PtableHead2)
colnames(HplayerInformationAndRecordAll) = c(HtableHead1,HtableHead2)
# データを保存したい場合コメントを外す
#write.csv(PplayerInformationAndRecordAll, "PplayerInformationAndRecordAll.csv")
#write.csv(HplayerInformationAndRecordAll, "HplayerInformationAndRecordAll.csv")
# うえで保存したデータを読み込みたい場合はコメントを外す
#HplayerInformationAndRecordAll = fread("HplayerInformationAndRecordAll.csv", encoding="UTF-8")
#PplayerInformationAndRecordAll = fread("PplayerInformationAndRecordAll.csv", encoding="UTF-8")
#HplayerInformationAndRecordAll = HplayerInformationAndRecordAll[,-1]
#PplayerInformationAndRecordAll = PplayerInformationAndRecordAll[,-1]
# もし重複がとれてしまった場合はコメントを外して重複削除を行う
#PplayerInformationAndRecordAll= PplayerInformationAndRecordAll[!duplicated(PplayerInformationAndRecordAll$名前), ]
#HplayerInformationAndRecordAll = HplayerInformationAndRecordAll [!duplicated(HplayerInformationAndRecordAll$名前), ]
# 必要なデータのみ使う
# 年棒と関係なさそうなデータは省いている
dataForLinearRegressionH = HplayerInformationAndRecordAll[,c("球団","ポジション","年齢","年俸","試合","打席数","打数","得点","安打","二塁打","三塁打","本塁打","塁打","打点","盗塁","盗塁刺","犠打","犠飛","四球","死球","三振","併殺打","打率","長打率","出塁率","OPS")]
dataForLinearRegressionP = PplayerInformationAndRecordAll[,c("球団","年齢","年俸","試合","勝利","敗北","セlブ","完投","完封勝","無四球","打者","投球回","被安打","被本塁打","与四球","与死球","奪三振","暴投","ボlク","失点","自責点","防御率","WHIP")]
dataForLinearRegressionH = as.matrix(dataForLinearRegressionH)
dataForLinearRegressionP = as.matrix(dataForLinearRegressionP)
# 不要な部分を削除する(「歳」とか「万円」とか)
## コピペ良くない…
dataForLinearRegressionH[,"年齢"] = gsub("歳","",dataForLinearRegressionH[,"年齢"])
dataForLinearRegressionH[,"年俸"] = gsub("万円（推定）","",dataForLinearRegressionH[,"年俸"])
dataForLinearRegressionH[,"年俸"] = gsub(",","",dataForLinearRegressionH[,"年俸"])
dataForLinearRegressionP[,"年齢"] = gsub("歳","",dataForLinearRegressionP[,"年齢"])
dataForLinearRegressionP[,"年俸"] = gsub("万円（推定）","",dataForLinearRegressionP[,"年俸"])
dataForLinearRegressionP[,"年俸"] = gsub(",","",dataForLinearRegressionP[,"年俸"])
# 線形回帰用のデータにする
## これをしないと数値として処理してくれず，文字列と認識されてしまったので
## もっと良い方法あったら教えてほしい
dataH = c()
for (i in 3:ncol(dataForLinearRegressionH)){
dataH = cbind(dataH, as.numeric(dataForLinearRegressionH[,i]))
}
colnames(dataH) = c("年齢","年俸","試合","打席数","打数","得点","安打","二塁打","三塁打","本塁打","塁打","打点","盗塁","盗塁刺","犠打","犠飛","四球","死球","三振","併殺打","打率","長打率","出塁率","OPS")
Yh = dataH[,2]
Xh= dataH[,-2]
# パリーグセリーグに分けるなら…
#dataHCe= dataH[1:117,]
#dataHPa= dataH[118:nrow(dataH),]
# やっと線形回帰
resultH = lm(Yh ~ Xh+as.factor(dataForLinearRegressionH[,1])+as.factor(dataForLinearRegressionH[,2]))
# 対数正規分布を仮定して一般化線形回帰（GLM）
resultHglm = glm(Yh ~ Xh+as.factor(dataForLinearRegressionH[,1])+as.factor(dataForLinearRegressionH[,2]),family = gaussian(link = "log"))
## コピペ良くない…
dataP = c()
for (i in 2:ncol(dataForLinearRegressionP)){
dataP = cbind(dataP, as.numeric(dataForLinearRegressionP[,i]))
}
colnames(dataP) = c("年齢","年俸","試合","勝利","敗北","セlブ","完投","完封勝","無四球","打者","投球回","被安打","被本塁打","与四球","与死球","奪三振","暴投","ボlク","失点","自責点","防御率","WHIP")
Yp = dataP[,2]
Xp = dataP[,-2]
# 線形回帰
resultP = lm(Yp ~ Xp+as.factor(dataForLinearRegressionP[,1]))
# 対数正規分布を仮定して一般化線形回帰（GLM）
resultPglm = glm(Yp ~ Xp+as.factor(dataForLinearRegressionP[,1]),family = gaussian(link = "log"))
# 残差（推定値との差）を取得して、選手名を付け加える
# その後昇順に並べる
# 推定値との差を取った時に，小さい順からならべたもの
# 前半は，推定値（本来このぐらいもらっているだろう値）よりも低い人が並ぶ，つまり「もっと貰って良い人」
# 後半は，推定値（本来このぐらいもらっているだろう値）よりも高い人が並ぶ，つまり「貰いすぎじゃない？な人」
residualsP = residuals(resultP)
residualsPWithName = cbind(PplayerInformationAndRecordAll[,1], residualsP)
orderedResidualsPWithName = residualsPWithName[order(residualsPWithName[,2]),]
residualsH = residuals(resultH)
residualsHWithName = cbind(HplayerInformationAndRecordAll[,1], residualsH)
orderedResidualsHWithName = residualsHWithName[order(residualsHWithName[,2]),]
# 一般化線形回帰（GLM）をした場合の残差を取得
residualsHglm = residuals(resultHglm)
residualsPglm = residuals(resultPglm)
residualsglmPWithName = cbind(PplayerInformationAndRecordAll[,1], residualsPglm)
residualsglmHWithName = cbind(HplayerInformationAndRecordAll[,1], residualsHglm)
orderedResidualsglmPWithName = residualsglmPWithName[order(residualsglmPWithName[,2]),]
orderedResidualsglmHWithName = residualsglmHWithName[order(residualsglmHWithName[,2]),]
library("rvest", lib.loc="/Library/Frameworks/R.framework/Versions/3.5/Resources/library")
#　広島の野手データのあるページを取得
cURLforH = read_html("http://baseball-data.com/ranking-salary/c/h.html")
#　広島の投手データのあるページを取得
cURLforP = read_html("http://baseball-data.com/ranking-salary/c/p.html")
# 2秒停止
Sys.sleep(2)
# 広島の野手のURLを一覧で取得
urlListForCH = cURLforH %>% html_nodes(xpath = "/html/body/div[@id='container']/div[@id='main']/table[@id='tbl']/tbody//a") %>% html_attr("href")
# 広島の投手のURLを一覧で取得
urlListForCP = cURLforP %>% html_nodes(xpath = "/html/body/div[@id='container']/div[@id='main']/table[@id='tbl']/tbody//a") %>% html_attr("href")
#　巨人のデータ
gURLforH = read_html("http://baseball-data.com/ranking-salary/g/h.html")
gURLforP = read_html("http://baseball-data.com/ranking-salary/g/p.html")
Sys.sleep(2)
urlListForGH = gURLforH %>% html_nodes(xpath = "/html/body/div[@id='container']/div[@id='main']/table[@id='tbl']/tbody//a") %>% html_attr("href")
urlListForGP = gURLforP %>% html_nodes(xpath = "/html/body/div[@id='container']/div[@id='main']/table[@id='tbl']/tbody//a") %>% html_attr("href")
# 横浜のデータ
ybURLforH = read_html("http://baseball-data.com/ranking-salary/yb/h.html")
ybURLforP = read_html("http://baseball-data.com/ranking-salary/yb/p.html")
Sys.sleep(2)
urlListForYbH = ybURLforH %>% html_nodes(xpath = "/html/body/div[@id='container']/div[@id='main']/table[@id='tbl']/tbody//a") %>% html_attr("href")
urlListForYbP = ybURLforP %>% html_nodes(xpath = "/html/body/div[@id='container']/div[@id='main']/table[@id='tbl']/tbody//a") %>% html_attr("href")
# 阪神のデータ
tURLforH = read_html("http://baseball-data.com/ranking-salary/t/h.html")
tURLforP = read_html("http://baseball-data.com/ranking-salary/t/p.html")
Sys.sleep(2)
urlListForTH = tURLforH %>% html_nodes(xpath = "/html/body/div[@id='container']/div[@id='main']/table[@id='tbl']/tbody//a") %>% html_attr("href")
urlListForTP = tURLforP %>% html_nodes(xpath = "/html/body/div[@id='container']/div[@id='main']/table[@id='tbl']/tbody//a") %>% html_attr("href")
# ヤクルトのデータ
sURLforH = read_html("http://baseball-data.com/ranking-salary/s/h.html")
sURLforP = read_html("http://baseball-data.com/ranking-salary/s/p.html")
Sys.sleep(2)
urlListForSH = sURLforH %>% html_nodes(xpath = "/html/body/div[@id='container']/div[@id='main']/table[@id='tbl']/tbody//a") %>% html_attr("href")
urlListForSP = sURLforP %>% html_nodes(xpath = "/html/body/div[@id='container']/div[@id='main']/table[@id='tbl']/tbody//a") %>% html_attr("href")
# 中日のデータ
dURLforH = read_html("http://baseball-data.com/ranking-salary/g/h.html")
dURLforP = read_html("http://baseball-data.com/ranking-salary/g/p.html")
Sys.sleep(2)
urlListForDH = dURLforH %>% html_nodes(xpath = "/html/body/div[@id='container']/div[@id='main']/table[@id='tbl']/tbody//a") %>% html_attr("href")
urlListForDP = dURLforP %>% html_nodes(xpath = "/html/body/div[@id='container']/div[@id='main']/table[@id='tbl']/tbody//a") %>% html_attr("href")
# 日本ハムのデータ
fURLforH = read_html("http://baseball-data.com/ranking-salary/f/h.html")
fURLforP = read_html("http://baseball-data.com/ranking-salary/f/p.html")
Sys.sleep(2)
urlListForFH = fURLforH %>% html_nodes(xpath = "/html/body/div[@id='container']/div[@id='main']/table[@id='tbl']/tbody//a") %>% html_attr("href")
urlListForFP = fURLforP %>% html_nodes(xpath = "/html/body/div[@id='container']/div[@id='main']/table[@id='tbl']/tbody//a") %>% html_attr("href")
# ソフトバンクのデータ
hURLforH = read_html("http://baseball-data.com/ranking-salary/h/h.html")
hURLforP = read_html("http://baseball-data.com/ranking-salary/h/p.html")
Sys.sleep(2)
urlListForHH = hURLforH %>% html_nodes(xpath = "/html/body/div[@id='container']/div[@id='main']/table[@id='tbl']/tbody//a") %>% html_attr("href")
urlListForHP = hURLforP %>% html_nodes(xpath = "/html/body/div[@id='container']/div[@id='main']/table[@id='tbl']/tbody//a") %>% html_attr("href")
# ロッテのデータ
mURLforH = read_html("http://baseball-data.com/ranking-salary/m/h.html")
mURLforP = read_html("http://baseball-data.com/ranking-salary/m/p.html")
Sys.sleep(2)
urlListForMH = mURLforH %>% html_nodes(xpath = "/html/body/div[@id='container']/div[@id='main']/table[@id='tbl']/tbody//a") %>% html_attr("href")
urlListForMP = mURLforP %>% html_nodes(xpath = "/html/body/div[@id='container']/div[@id='main']/table[@id='tbl']/tbody//a") %>% html_attr("href")
# 楽天のデータ
eURLforH = read_html("http://baseball-data.com/ranking-salary/e/h.html")
eURLforP = read_html("http://baseball-data.com/ranking-salary/e/p.html")
Sys.sleep(2)
urlListForEH = eURLforH %>% html_nodes(xpath = "/html/body/div[@id='container']/div[@id='main']/table[@id='tbl']/tbody//a") %>% html_attr("href")
urlListForEP = eURLforP %>% html_nodes(xpath = "/html/body/div[@id='container']/div[@id='main']/table[@id='tbl']/tbody//a") %>% html_attr("href")
# 西武のデータ
lURLforH = read_html("http://baseball-data.com/ranking-salary/l/h.html")
lURLforP = read_html("http://baseball-data.com/ranking-salary/l/p.html")
Sys.sleep(2)
urlListForLH = lURLforH %>% html_nodes(xpath = "/html/body/div[@id='container']/div[@id='main']/table[@id='tbl']/tbody//a") %>% html_attr("href")
urlListForLP = lURLforP %>% html_nodes(xpath = "/html/body/div[@id='container']/div[@id='main']/table[@id='tbl']/tbody//a") %>% html_attr("href")
# オリックスのデータ
bsURLforH = read_html("http://baseball-data.com/ranking-salary/bs/h.html")
bsRLforP = read_html("http://baseball-data.com/ranking-salary/bs/p.html")
Sys.sleep(2)
urlListForBsH = bsURLforH %>% html_nodes(xpath = "/html/body/div[@id='container']/div[@id='main']/table[@id='tbl']/tbody//a") %>% html_attr("href")
urlListForBsP = bsRLforP %>% html_nodes(xpath = "/html/body/div[@id='container']/div[@id='main']/table[@id='tbl']/tbody//a") %>% html_attr("href")
# 野手のデータをまとめる
urlListAllH = c (urlListForCH, urlListForGH,urlListForYbH,urlListForTH,urlListForSH,urlListForDH, urlListForFH, urlListForHH,urlListForMH, urlListForEH, urlListForLH,urlListForBsH)
# 投手のデータをまとめる
urlListAllP = c (urlListForCP, urlListForGP,urlListForYbP,urlListForTP,urlListForSP,urlListForDP, urlListForFP, urlListForHP,urlListForMP, urlListForEP, urlListForLP,urlListForBsP)
# 野手データの人数
numberOfAllHPlayers = urlListAllH %>% length
# 投手データの人数
numberOfAllPPlayers = urlListAllP %>% length
## 先ほどまとめた野手URLから，それぞれ2016年の成績を取得
playerInformationAndRecordAll = c()
for (i in 1:numberOfAllHPlayers){
tmphtml = read_html(urlListAllH[i])
recordList = tmphtml %>% html_nodes(xpath = "/html/body/div[@id='container']/div[@id='main']/table[@id='tbl']/tbody//tr")
numberOfRecordList = recordList %>% length
for (j in 1:numberOfRecordList){
if(identical(grep("<td style=\"text-align:center;\">2016",recordList[j]), integer(0))){next}
lastYearRecord = recordList[j] %>%  html_nodes("td") %>% html_text() %>% as.vector()
playerName = tmphtml %>% html_nodes(xpath = "/html/body/div[@id='container']/div[@id='main']/h2[@class='pname']") %>% html_text()
playerInformation = tmphtml %>% html_nodes(xpath = "/html/body/div[@id='container']/div[@id='main']/table[@class='player-info']//td") %>% html_text() %>% as.vector()
playerInformationAndRecord = c(playerName, playerInformation, lastYearRecord) %>% as.matrix()
}
# 2016年のデータが無い場合はスキップする
if (!is.null(playerInformationAndRecord)){
playerInformationAndRecordAll = cbind(playerInformationAndRecordAll, playerInformationAndRecord)
Sys.sleep(2)
}
}
orderedResidualsglmPWithName
urlListAllH
## 先ほどまとめた野手URLから，それぞれ2016年の成績を取得
playerInformationAndRecordAll = c()
for (i in 1:numberOfAllHPlayers){
tmphtml = read_html(urlListAllH[i])
recordList = tmphtml %>% html_nodes(xpath = "/html/body/div[@id='container']/div[@id='main']/table[@id='tbl']/tbody//tr")
numberOfRecordList = recordList %>% length
for (j in 1:numberOfRecordList){
if(identical(grep("<td style=\"text-align:center;\">2016",recordList[j]), integer(0))){next}
lastYearRecord = recordList[j] %>%  html_nodes("td") %>% html_text() %>% as.vector()
playerName = tmphtml %>% html_nodes(xpath = "/html/body/div[@id='container']/div[@id='main']/h2[@class='pname']") %>% html_text()
playerInformation = tmphtml %>% html_nodes(xpath = "/html/body/div[@id='container']/div[@id='main']/table[@class='player-info']//td") %>% html_text() %>% as.vector()
playerInformationAndRecord = c(playerName, playerInformation, lastYearRecord) %>% as.matrix()
}
# 2016年のデータが無い場合はスキップする
if (!is.null(playerInformationAndRecord)){
playerInformationAndRecordAll = cbind(playerInformationAndRecordAll, playerInformationAndRecord)
Sys.sleep(2)
}
}
playerInformationAndRecordAll = c()
for (i in 1:numberOfAllHPlayers){
tmphtml = read_html(urlListAllH[i])
recordList = tmphtml %>% html_nodes(xpath = "/html/body/div[@id='container']/div[@id='main']/table[@id='tbl']/tbody//tr")
numberOfRecordList = recordList %>% length
for (j in 1:numberOfRecordList){
if(identical(grep("<td style=\"text-align:center;\">2016",recordList[j]), integer(0))){next}
lastYearRecord = recordList[j] %>%  html_nodes("td") %>% html_text() %>% as.vector()
playerName = tmphtml %>% html_nodes(xpath = "/html/body/div[@id='container']/div[@id='main']/h2[@class='pname']") %>% html_text()
playerInformation = tmphtml %>% html_nodes(xpath = "/html/body/div[@id='container']/div[@id='main']/table[@class='player-info']//td") %>% html_text() %>% as.vector()
playerInformationAndRecord = c(playerName, playerInformation, lastYearRecord) %>% as.matrix()
}
# 2016年のデータが無い場合はスキップする
if (!is.null(playerInformationAndRecord)){
playerInformationAndRecordAll = cbind(playerInformationAndRecordAll, playerInformationAndRecord)
Sys.sleep(2)
}
}
playerInformationAndRecordAll = c()
for (i in 1:numberOfAllHPlayers){
tmphtml = read_html(urlListAllH[i])
recordList = tmphtml %>% html_nodes(xpath = "/html/body/div[@id='container']/div[@id='main']/table[@id='tbl']/tbody//tr")
numberOfRecordList = recordList %>% length
for (j in 1:numberOfRecordList){
if(identical(grep("<td style=\"text-align:center;\">2016",recordList[j]), integer(0))){next}
lastYearRecord = recordList[j] %>%  html_nodes("td") %>% html_text() %>% as.vector()
playerName = tmphtml %>% html_nodes(xpath = "/html/body/div[@id='container']/div[@id='main']/h2[@class='pname']") %>% html_text()
playerInformation = tmphtml %>% html_nodes(xpath = "/html/body/div[@id='container']/div[@id='main']/table[@class='player-info']//td") %>% html_text() %>% as.vector()
playerInformationAndRecord = c(playerName, playerInformation, lastYearRecord) %>% as.matrix()
}
# 2016年のデータが無い場合はスキップする
if (!is.null(playerInformationAndRecord)){
playerInformationAndRecordAll = cbind(playerInformationAndRecordAll, playerInformationAndRecord)
Sys.sleep(0.1)
}
}
install.packages("wle", dependencies=T)
install.packages("tidyverse")
library(tidyverse)
x <- c(1,2,3,2,7,5,9,1)
y <- c(14,20,21,15,36,27,40,8)
ans <- lm(y~x)
plot(x,y)
ines(x,fitted(ans),col="red")
lines(x,fitted(ans),col="red")
lines(x,fitted(ans),col="blue")
lines(x,fitted(ans),col="green")
lines(x,fitted(ans),col="silver")
lines(x,fitted(ans),col="yellow")
lines(x,fitted(ans),col="black")
f1 <- "sample1.png"
f1 <- "sample1.png"
f2 <- "sample2.jpeg"
png(f1, width = 800, height = 600)
plot(x,y)
lines(x,fitted(ans),col="red")
dev.off()
plot(x,y)
lines(x,fitted(ans),col="red")
dev.copy(jpeg,f2,width=900,height=675)
dev.off()
install.packages("rmarkdown")
cars
pressure
knitr::opts_chunk$set(echo = TRUE)
knitr
summary(cars)
knitr::opts_chunk$set(echo = TRUE)
p <- ("おいなりさん","おまわりさん","課長")
p <- ["おいなりさん","おまわりさん","課長"]
p <- c("おいなりさん","おまわりさん","課長")
p <- c("ないやん","許して","壊れる")
p
p
p <- c (1:10 by:5)
p <- c (1:10)
p
p[2] <- c(2:11)
p[2,] <- c(2:11)
p
summary(p)
knit_with_parameters('~/RStudio_Workspace/myfirstR/firstRMarkdown.Rmd')
knit_with_parameters('~/RStudio_Workspace/myfirstR/firstRMarkdown.Rmd')
# 	ñ@äwÉZÉ~ÉiÅ[Åuñ@ó•â∆ÇÃÇΩÇﬂÇÃé¿èÿï™êÕì¸ñÂÅvëÊ3âÒ É\Å[ÉXÉRÅ[Éh
#	(C) 2011 MORITA Hatsuru
library(lattice)
# Essay #3 reemployment data
reemp <- c(7.1, 3.5, 14.3, 6.3, 18.0, 5.4, 6.9, 9.2, 11.0, 7.7)
hist(reemp)
histogram(reemp)
densityplot(reemp)
mean(reemp)
median(reemp)
summary(reemp)
# two distributions
x <- seq(40,160,length=1000)
y <- dnorm(x, mean=100, sd=20)*400
z <- dnorm(x, mean=100, sd=10)*400
xyplot(y~x,
xlab = "x",
ylab = "pop",
ylim = c(-1,18),
panel = function(x,y){
panel.xyplot(x,y, type="l", col="blue", lty=2)
panel.xyplot(x,z, type="l", col="red", lty=1)
},
key = list(text=list(c("SD=20","SD=10")),
lines=list(col=c("blue","red"),lty=c(2,1)),
space="top", border=T
)
)
# not population, but sample!
var(reemp)
sd(reemp)
# reemployment + training data
training <- c(3,2,1,2,1,3,3,2,2,1)
reemptrain <- cbind(reemp, training)
xyplot(training~reemp,
xlab ="Reemployment",
ylab ="Training",
panel = function(x,y){
panel.xyplot(reemp,training, col="red", pch=4)
},
)
cor(reemptrain)
#not population, but sample!
cov(reemptrain)
group1 <- c(0.7,-1.6,-0.2,-1.2,-0.1,3.4,3.7,0.8,0.0,2.0)    # グループ 1 の睡眠時間の増加
group2 <- c(1.9, 0.8, 1.1, 0.1,-0.1,4.4,5.5,1.6,4.6,3.4)    # グループ 2 の睡眠時間の増加
group1 <- c(0.7,-1.6,-0.2,-1.2,-0.1,3.4,3.7,0.8,0.0,2.0)    # グループ 1 の睡眠時間の増加
group2 <- c(1.9, 0.8, 1.1, 0.1,-0.1,4.4,5.5,1.6,4.6,3.4)    # グループ 2 の睡眠時間の増加
t.test(group1, mu=3)
t.test(group1, mu=0.75)
t.test(group1, group2, var.equal=T)
plot(Volume ~ Girth, data = trees)
plot(Volume ~ Girth, data = trees)
plot(Volume ~ Girth, data = trees)
plot(Volume ~ Girth, data = trees)
plot(x ~ y, data=)
plot(x ~ y, data=)
plot(x ~ y, data=)
plot(x ~ y, data=)
plot(x ~ y, data=)
plot(x ~ y, data=)
plot(x ~ y, data=)
plot(x ~ y, data=)
plot(x ~ y, data=)
plot(x ~ y, data=)
plot(x ~ y, data=)
plot(x ~ y, data=)
plot(x ~ y, data=)
plot(x ~ y, data=)
plot(x ~ y, data=)
plot(x ~ y, data=)
plot(Volume ~ Girth, data = trees)
result <- lm(Volume ~ Girth, data = trees)
abline(result)
summary(result)
trees
iris
head(iris)
plot(Sepal.Length ~ Sepal.Width, data=iris)
result = lm(Sepal.Length ~ Sepal.Width, data=iris)
abline(result)
summary(result)
x <- trees$Height;  y <- trees$Volume
z <- lsfit(x, y)
print(z)
result <- lm(Volume ~ Girth, data = trees)                   # 回帰分析を行う
summary(result)                                              # 分析結果の要約
ls
di
dir
cd vsCodeWorkSpace
getwd()
cd vsCodeWorkspace
setwd("~/ticket_tenbai")
getwd()
data = read.csv("all-united.csv",sep=",",header=TRUE)
head(data)
data$name
data$after
result = lm(data$price_per_unit~data$left_days+data$after+data$is_jannies+data$after_and_js_jannies,data)
summary(result)
library(readr)
D_enka <- read_csv("CSV/演歌/演歌.csv")
View(D_enka)
head(D_enka)
result = lm(D_enka$price_per_unit~ D_enka$after+D_enka$left_days)
summary(resut)
summary(result)
library(readr)
only_jannies <- read_csv("only-jannies.csv")
View(only_jannies)
result = lm(only_jannies$price_per_unit~only_jannies$left_days+only_jannies$after,only_jannies)
summary(result)
library(readr)
randsampled_j <- read_csv("分析対象データ群/ランダムサンプル済みジャニーズ&演歌.csv")
View(randsampled_j)
head(randsampled_j)
result = lm(randsampled_j$price_per_unit~randsampled_j$left_days+randsampled_j$is_jannies+randsampled_j$after+randsampled_j$after_and_js_jannies)
summary(result)
write.csv(summary(result),file="ランダムサンプル一回目.csv")
write.csv(summary(result)$coef,file="ランダムサンプル一回目.csv")
View(only_jannies)
sample( 1:10, 8 )
size <- 200
times <- 20
y <- vector(length=size)
data <- vector(length=times)
for ( i in 1:size ) {
for( j in 1:times ) {
data[j] <- mean( sample( slength, i ) )
}
y[i] <- sd( data )
}
x <- 1:size
plot( x, y, xlab="sample size(N)", ylab="Standard Error", axes=F )
y <- sd(slength)/sqrt(x) # 理論値を代入
par( new=T ) # 理論値曲線を追加
plot( x, y, type="l", col="red", ylab="", xlab="", lwd=2 )
par( mfrow=c(1, 2) )
hist( katakana, right=F )
result <- qqnorm( katakana )
qqline( katakana, col="red", lwd=2 )
r <- round( cor( result$x, result$y ), 3 )
mtext( paste( "r=" r ) )
par( mfrow=c(1, 1) )
for (i in 1:501){}
for (i in 1:501){}
for (i in 1:501){
print(i)
}
for (i in 1:501){
print(i)
}
for (i in 1:11){
print(formatC(i,width=3,flag="0"))
}
for (i in 1:3){
print(formatC(i,width=3,flag="0"))
}
getwd()
data = read.csv("分析対象データ群/ジャニーズをランダムサンプルのち演歌と結合CSV/ジャニーズをランダムサンプルのち演歌と結合001回目.csv")
View(data)
for (i in 1:11){
i  = formatC(i,width = 3,flag="0")
data = read.csv("分析対象データ群/ジャニーズグループのみランダムサンプリングCSV/ジャニーズグループのみランダムサンプリング"+i+"回目.csv")
}
View(data)
View(data)
print("回目の呼び出し")
for (i in 1:11){
print(i)
print("回目の呼び出し")
i  = formatC(i,width = 3,flag="0")
path = paste("分析対象データ群/ジャニーズグループのみランダムサンプリングCSV/ジャニーズグループのみランダムサンプリング",i,"回目.csv",sep = "")
data = read.csv(path)
}
source('~/ticket_tenbai/Rスクリプト/ランダムサンプリング 済み結合CSVを連続で開いて重回帰分析.R')
source('~/ticket_tenbai/Rスクリプト/ランダムサンプリング 済み結合CSVを連続で開いて重回帰分析.R')
source('~/ticket_tenbai/Rスクリプト/ランダムサンプリング 済み結合CSVを連続で開いて重回帰分析.R')
source('~/ticket_tenbai/Rスクリプト/ランダムサンプリング 済み結合CSVを連続で開いて重回帰分析.R')
View(result)
View(only_jannies)
View(data)
View(D_enka)
source('~/ticket_tenbai/Rスクリプト/ランダムサンプリング 済み結合CSVを連続で開いて重回帰分析.R')
source('~/ticket_tenbai/Rスクリプト/ランダムサンプリング 済み結合CSVを連続で開いて重回帰分析.R')
data = read.csv("分析対象データ群/ジャニーズグループのみランダムサンプリングCSV/ジャニーズグループのみランダムサンプリング001回目.csv")
result = lm(data$price_per_unit~data$left_days+data$is_jannies+data$after+data$after_and_js_jannies,data=data)
summary(result)
write.csv(result,file = "分析対象データ群/Rによりランダムサンプリング済み結合を分析/result001回目.csv")
write.csv(summary(result),file = "分析対象データ群/Rによりランダムサンプリング済み結合を分析/result001回目.csv")
write.csv(result,"分析対象データ群/Rによりランダムサンプリング済み結合を分析/result001回目.csv")
write.table(result,"分析対象データ群/Rによりランダムサンプリング済み結合を分析/result001回目.csv")
write.table(summary(result),"分析対象データ群/Rによりランダムサンプリング済み結合を分析/result001回目.csv",sep=",")
sink("分析対象データ群/Rによりランダムサンプリング済み結合を分析/result001回目.csv")
summary(result)
sink()
source('~/ticket_tenbai/Rスクリプト/ランダムサンプリング 済み結合CSVを連続で開いて重回帰分析.R')
source('~/ticket_tenbai/Rスクリプト/ランダムサンプリング 済み結合CSVを連続で開いて重回帰分析.R')
source('~/ticket_tenbai/Rスクリプト/ランダムサンプリング 済み結合CSVを連続で開いて重回帰分析.R')
source('~/ticket_tenbai/Rスクリプト/ランダムサンプリング 済み結合CSVを連続で開いて重回帰分析.R')
source('~/ticket_tenbai/Rスクリプト/ランダムサンプリング 済み結合CSVを連続で開いて重回帰分析.R')
source('~/ticket_tenbai/Rスクリプト/ランダムサンプリング 済み結合CSVを連続で開いて重回帰分析.R')
source('~/ticket_tenbai/Rスクリプト/ランダムサンプリング 済み結合CSVを連続で開いて重回帰分析.R')
source('~/ticket_tenbai/Rスクリプト/ランダムサンプリング 済み結合CSVを連続で開いて重回帰分析.R')
source('~/ticket_tenbai/Rスクリプト/ランダムサンプリング 済み結合CSVを連続で開いて重回帰分析.R')
source('~/ticket_tenbai/Rスクリプト/ランダムサンプリング 済み結合CSVを連続で開いて重回帰分析.R')
source('~/ticket_tenbai/Rスクリプト/ランダムサンプリング 済み結合CSVを連続で開いて重回帰分析.R')
source('~/ticket_tenbai/Rスクリプト/ランダムサンプリング 済み結合CSVを連続で開いて重回帰分析.R')
source('~/ticket_tenbai/Rスクリプト/ランダムサンプリング 済み結合CSVを連続で開いて重回帰分析.R')
source('~/ticket_tenbai/Rスクリプト/ランダムサンプリング 済み結合CSVを連続で開いて重回帰分析.R')
source('~/ticket_tenbai/Rスクリプト/ランダムサンプリング 済み結合CSVを連続で開いて重回帰分析.R')
source('~/ticket_tenbai/Rスクリプト/ランダムサンプリング 済み結合CSVを連続で開いて重回帰分析.R')
